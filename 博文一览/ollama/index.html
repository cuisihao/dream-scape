<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="什么是ollama Ollama 是一个开源软件，让用户可以在自己的硬件上运行、创建和分享大型语言模型服务。这个平台适合希望在本地端运行模型的使用者，因为它不仅可以保护隐私，还允许用户通过命令行界面轻松地设置和互动。Ollama 支持包括 Llama 2 和 Mistral 等多种模型，并提供灵活的定制选项，例如从其他格式导入模型并设置运行参数。
自从chatgpt出世以来，世界上各个大模型就像雨后春笋一般，遍地开花。但是无独有偶的是每一个大模型都是需要注册他们的账号，或网页端，或app中接受他们提供的服务。
我们自己要如何在本地使用这些大模型呢。ollama就在这时孕育而生了。只需要一道指令便可以轻松运行大模型。
下面我们将从ollama&#43;图形界面Open WebUI来实现本地运行类chatgpt服务。
安装ollama 首先我们需要到ollama官网下载该软件，或者github中https://github.com/ollama/ollama
Ollama支持Linux、macOS、Windows、Raspberry Pi OS。
建议电脑至少CPUi5七代以上，RAM 8GB以上再使用Ollama。
因为Ollama使用llama.cpp技术，所以不需要独立GPU也能跑。不过有独立GPU更好，可以将一些模型层offload给GPU加速运算。
根据指引下载ollama完成安装之后，可以在cmd查看是否成功安装了ollama
这里我安装是0.1.48版本 如果不小心关闭了ollama程序，输入ollama serve即可重新启动ollama服务，ollama自带运行日志管理。
部署open-webui Open WebUI 是一种可扩展、功能丰富且用户友好的自托管 WebUI，旨在完全离线运行。它支持各种LLM运行器，包括 Ollama 和 OpenAI 兼容的 API。
地址：https://github.com/open-webui/open-webui/tree/main
我这里是使用docker来进行安装的，如果本机没有docker的话请自行去下载docker 地址：https://www.docker.com/products/docker-desktop/ docker安装完成之后，只需要open-webui官方提供的安装方法进行安装即可
因为我本机已经安装了ollama 所以直接在cmd中执行
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main 这个时候在docker desktop的Images中就能看到下载的open webui了
加载大模型 我们在docker中点击run启动open-webui项目后在浏览器中输入http://localhost:3000/即可进入
open-webui本身支持各种语言，英语困难的小伙伴也不用担心。点击右上角我的，设置里面可以选择语言，和主题风格。
设置语言之后，我们在下面的管理员设置中可以管理我的大模型
但是本人不建议在这里去下载我们的大模型，因为在这里下载会遇到卡在百分之99的情况。来自某人的教训。我们可以使用ollama提供的命令交互的方式去加载我们的大模型。
我们只需要在终端执行 便可以加载我们需要的大模型
ollama run llama3:8b 当然我们可以在ollama中选择我们想要下载那些大模型 https://ollama.com/library
当我们加载完成大模型之后，我们就可以在本地愉快的和大模型进行对话了。" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://csh-dream-scape.pages.dev/%E5%8D%9A%E6%96%87%E4%B8%80%E8%A7%88/ollama/" />


    <title>
        
            Ollama ＋ Open WebUI，快捷部署AI大模型到电脑,并离线执行 :: csh.blog  — blog.x
        
    </title>





<link rel="stylesheet" href="/main.949191c1dcc9c4a887997048b240354e47152016d821198f89448496ba42e491.css" integrity="sha256-lJGRwdzJxKiHmXBIskA1TkcVIBbYIRmPiUSElrpC5JE=">



    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">



  <meta itemprop="name" content="Ollama ＋ Open WebUI，快捷部署AI大模型到电脑,并离线执行">
  <meta itemprop="description" content="什么是ollama Ollama 是一个开源软件，让用户可以在自己的硬件上运行、创建和分享大型语言模型服务。这个平台适合希望在本地端运行模型的使用者，因为它不仅可以保护隐私，还允许用户通过命令行界面轻松地设置和互动。Ollama 支持包括 Llama 2 和 Mistral 等多种模型，并提供灵活的定制选项，例如从其他格式导入模型并设置运行参数。
自从chatgpt出世以来，世界上各个大模型就像雨后春笋一般，遍地开花。但是无独有偶的是每一个大模型都是需要注册他们的账号，或网页端，或app中接受他们提供的服务。
我们自己要如何在本地使用这些大模型呢。ollama就在这时孕育而生了。只需要一道指令便可以轻松运行大模型。
下面我们将从ollama&#43;图形界面Open WebUI来实现本地运行类chatgpt服务。
安装ollama 首先我们需要到ollama官网下载该软件，或者github中https://github.com/ollama/ollama
Ollama支持Linux、macOS、Windows、Raspberry Pi OS。
建议电脑至少CPUi5七代以上，RAM 8GB以上再使用Ollama。
因为Ollama使用llama.cpp技术，所以不需要独立GPU也能跑。不过有独立GPU更好，可以将一些模型层offload给GPU加速运算。
根据指引下载ollama完成安装之后，可以在cmd查看是否成功安装了ollama
这里我安装是0.1.48版本 如果不小心关闭了ollama程序，输入ollama serve即可重新启动ollama服务，ollama自带运行日志管理。
部署open-webui Open WebUI 是一种可扩展、功能丰富且用户友好的自托管 WebUI，旨在完全离线运行。它支持各种LLM运行器，包括 Ollama 和 OpenAI 兼容的 API。
地址：https://github.com/open-webui/open-webui/tree/main
我这里是使用docker来进行安装的，如果本机没有docker的话请自行去下载docker 地址：https://www.docker.com/products/docker-desktop/ docker安装完成之后，只需要open-webui官方提供的安装方法进行安装即可
因为我本机已经安装了ollama 所以直接在cmd中执行
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main 这个时候在docker desktop的Images中就能看到下载的open webui了
加载大模型 我们在docker中点击run启动open-webui项目后在浏览器中输入http://localhost:3000/即可进入
open-webui本身支持各种语言，英语困难的小伙伴也不用担心。点击右上角我的，设置里面可以选择语言，和主题风格。
设置语言之后，我们在下面的管理员设置中可以管理我的大模型
但是本人不建议在这里去下载我们的大模型，因为在这里下载会遇到卡在百分之99的情况。来自某人的教训。我们可以使用ollama提供的命令交互的方式去加载我们的大模型。
我们只需要在终端执行 便可以加载我们需要的大模型
ollama run llama3:8b 当然我们可以在ollama中选择我们想要下载那些大模型 https://ollama.com/library
当我们加载完成大模型之后，我们就可以在本地愉快的和大模型进行对话了。">
  <meta itemprop="datePublished" content="2024-07-12T16:26:54+08:00">
  <meta itemprop="dateModified" content="2024-07-12T16:26:54+08:00">
  <meta itemprop="wordCount" content="70">
  <meta itemprop="image" content="https://csh-dream-scape.pages.dev/">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://csh-dream-scape.pages.dev/">
  <meta name="twitter:title" content="Ollama ＋ Open WebUI，快捷部署AI大模型到电脑,并离线执行">
  <meta name="twitter:description" content="什么是ollama Ollama 是一个开源软件，让用户可以在自己的硬件上运行、创建和分享大型语言模型服务。这个平台适合希望在本地端运行模型的使用者，因为它不仅可以保护隐私，还允许用户通过命令行界面轻松地设置和互动。Ollama 支持包括 Llama 2 和 Mistral 等多种模型，并提供灵活的定制选项，例如从其他格式导入模型并设置运行参数。
自从chatgpt出世以来，世界上各个大模型就像雨后春笋一般，遍地开花。但是无独有偶的是每一个大模型都是需要注册他们的账号，或网页端，或app中接受他们提供的服务。
我们自己要如何在本地使用这些大模型呢。ollama就在这时孕育而生了。只需要一道指令便可以轻松运行大模型。
下面我们将从ollama&#43;图形界面Open WebUI来实现本地运行类chatgpt服务。
安装ollama 首先我们需要到ollama官网下载该软件，或者github中https://github.com/ollama/ollama
Ollama支持Linux、macOS、Windows、Raspberry Pi OS。
建议电脑至少CPUi5七代以上，RAM 8GB以上再使用Ollama。
因为Ollama使用llama.cpp技术，所以不需要独立GPU也能跑。不过有独立GPU更好，可以将一些模型层offload给GPU加速运算。
根据指引下载ollama完成安装之后，可以在cmd查看是否成功安装了ollama
这里我安装是0.1.48版本 如果不小心关闭了ollama程序，输入ollama serve即可重新启动ollama服务，ollama自带运行日志管理。
部署open-webui Open WebUI 是一种可扩展、功能丰富且用户友好的自托管 WebUI，旨在完全离线运行。它支持各种LLM运行器，包括 Ollama 和 OpenAI 兼容的 API。
地址：https://github.com/open-webui/open-webui/tree/main
我这里是使用docker来进行安装的，如果本机没有docker的话请自行去下载docker 地址：https://www.docker.com/products/docker-desktop/ docker安装完成之后，只需要open-webui官方提供的安装方法进行安装即可
因为我本机已经安装了ollama 所以直接在cmd中执行
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main 这个时候在docker desktop的Images中就能看到下载的open webui了
加载大模型 我们在docker中点击run启动open-webui项目后在浏览器中输入http://localhost:3000/即可进入
open-webui本身支持各种语言，英语困难的小伙伴也不用担心。点击右上角我的，设置里面可以选择语言，和主题风格。
设置语言之后，我们在下面的管理员设置中可以管理我的大模型
但是本人不建议在这里去下载我们的大模型，因为在这里下载会遇到卡在百分之99的情况。来自某人的教训。我们可以使用ollama提供的命令交互的方式去加载我们的大模型。
我们只需要在终端执行 便可以加载我们需要的大模型
ollama run llama3:8b 当然我们可以在ollama中选择我们想要下载那些大模型 https://ollama.com/library
当我们加载完成大模型之后，我们就可以在本地愉快的和大模型进行对话了。">



    <meta property="og:url" content="https://csh-dream-scape.pages.dev/%E5%8D%9A%E6%96%87%E4%B8%80%E8%A7%88/ollama/">
  <meta property="og:site_name" content="csh.blog">
  <meta property="og:title" content="Ollama ＋ Open WebUI，快捷部署AI大模型到电脑,并离线执行">
  <meta property="og:description" content="什么是ollama Ollama 是一个开源软件，让用户可以在自己的硬件上运行、创建和分享大型语言模型服务。这个平台适合希望在本地端运行模型的使用者，因为它不仅可以保护隐私，还允许用户通过命令行界面轻松地设置和互动。Ollama 支持包括 Llama 2 和 Mistral 等多种模型，并提供灵活的定制选项，例如从其他格式导入模型并设置运行参数。
自从chatgpt出世以来，世界上各个大模型就像雨后春笋一般，遍地开花。但是无独有偶的是每一个大模型都是需要注册他们的账号，或网页端，或app中接受他们提供的服务。
我们自己要如何在本地使用这些大模型呢。ollama就在这时孕育而生了。只需要一道指令便可以轻松运行大模型。
下面我们将从ollama&#43;图形界面Open WebUI来实现本地运行类chatgpt服务。
安装ollama 首先我们需要到ollama官网下载该软件，或者github中https://github.com/ollama/ollama
Ollama支持Linux、macOS、Windows、Raspberry Pi OS。
建议电脑至少CPUi5七代以上，RAM 8GB以上再使用Ollama。
因为Ollama使用llama.cpp技术，所以不需要独立GPU也能跑。不过有独立GPU更好，可以将一些模型层offload给GPU加速运算。
根据指引下载ollama完成安装之后，可以在cmd查看是否成功安装了ollama
这里我安装是0.1.48版本 如果不小心关闭了ollama程序，输入ollama serve即可重新启动ollama服务，ollama自带运行日志管理。
部署open-webui Open WebUI 是一种可扩展、功能丰富且用户友好的自托管 WebUI，旨在完全离线运行。它支持各种LLM运行器，包括 Ollama 和 OpenAI 兼容的 API。
地址：https://github.com/open-webui/open-webui/tree/main
我这里是使用docker来进行安装的，如果本机没有docker的话请自行去下载docker 地址：https://www.docker.com/products/docker-desktop/ docker安装完成之后，只需要open-webui官方提供的安装方法进行安装即可
因为我本机已经安装了ollama 所以直接在cmd中执行
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main 这个时候在docker desktop的Images中就能看到下载的open webui了
加载大模型 我们在docker中点击run启动open-webui项目后在浏览器中输入http://localhost:3000/即可进入
open-webui本身支持各种语言，英语困难的小伙伴也不用担心。点击右上角我的，设置里面可以选择语言，和主题风格。
设置语言之后，我们在下面的管理员设置中可以管理我的大模型
但是本人不建议在这里去下载我们的大模型，因为在这里下载会遇到卡在百分之99的情况。来自某人的教训。我们可以使用ollama提供的命令交互的方式去加载我们的大模型。
我们只需要在终端执行 便可以加载我们需要的大模型
ollama run llama3:8b 当然我们可以在ollama中选择我们想要下载那些大模型 https://ollama.com/library
当我们加载完成大模型之后，我们就可以在本地愉快的和大模型进行对话了。">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="博文一览">
    <meta property="article:published_time" content="2024-07-12T16:26:54+08:00">
    <meta property="article:modified_time" content="2024-07-12T16:26:54+08:00">
    <meta property="og:image" content="https://csh-dream-scape.pages.dev/">






    <meta property="article:published_time" content="2024-07-12 16:26:54 &#43;0800 CST" />











    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                csh.blog</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="/%E5%8D%9A%E6%96%87%E4%B8%80%E8%A7%88">Blog</a></li>
        <div class="submenu">
            <li class="dropdown">
                <a href="javascript:void(0)" class="dropbtn">en</a>
                <div class="dropdown-content">
                    
                </div>
            </li>
        </div>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
                <span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
   <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
   3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
   13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
 </svg></span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://csh-dream-scape.pages.dev/%E5%8D%9A%E6%96%87%E4%B8%80%E8%A7%88/ollama/">Ollama ＋ Open WebUI，快捷部署AI大模型到电脑,并离线执行</a></h2>

            
            
            

            <div class="post-content">
                <h2 id="什么是ollama"><strong>什么是ollama</strong></h2>
<p><img src="/images/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240712163146.png"></p>
<p>Ollama 是一个开源软件，让用户可以在自己的硬件上运行、创建和分享大型语言模型服务。这个平台适合希望在本地端运行模型的使用者，因为它不仅可以保护隐私，还允许用户通过命令行界面轻松地设置和互动。Ollama 支持包括 Llama 2 和 Mistral 等多种模型，并提供灵活的定制选项，例如从其他格式导入模型并设置运行参数。</p>
<p>自从chatgpt出世以来，世界上各个大模型就像雨后春笋一般，遍地开花。但是无独有偶的是每一个大模型都是需要注册他们的账号，或网页端，或app中接受他们提供的服务。</p>
<p>我们自己要如何在本地使用这些大模型呢。ollama就在这时孕育而生了。只需要一道指令便可以轻松运行大模型。</p>
<p>下面我们将从ollama+图形界面Open WebUI来实现本地运行类chatgpt服务。</p>
<h2 id="安装ollama"><strong>安装ollama</strong></h2>
<p>首先我们需要到ollama官网下载该软件，或者github中<a href="https:">https://github.com/ollama/ollama</a></p>
<p>Ollama支持Linux、macOS、Windows、Raspberry Pi OS。</p>
<p>建议电脑至少CPUi5七代以上，RAM 8GB以上再使用Ollama。</p>
<p>因为Ollama使用llama.cpp技术，所以不需要独立GPU也能跑。不过有独立GPU更好，可以将一些模型层offload给GPU加速运算。</p>
<p>根据指引下载ollama完成安装之后，可以在cmd查看是否成功安装了ollama</p>
<p><img src="/images/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240712170213.png"></p>
<p>这里我安装是0.1.48版本
如果不小心关闭了ollama程序，输入ollama serve即可重新启动ollama服务，ollama自带运行日志管理。</p>
<h2 id="部署open-webui"><strong>部署open-webui</strong></h2>
<p>Open WebUI 是一种可扩展、功能丰富且用户友好的自托管 WebUI，旨在完全离线运行。它支持各种LLM运行器，包括 Ollama 和 OpenAI 兼容的 API。</p>
<p>地址：<a href="https:">https://github.com/open-webui/open-webui/tree/main</a></p>
<p>我这里是使用docker来进行安装的，如果本机没有docker的话请自行去下载docker  地址：<a href="https:">https://www.docker.com/products/docker-desktop/</a>
docker安装完成之后，只需要open-webui官方提供的安装方法进行安装即可</p>
<p><img src="/images/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240712165650.png"></p>
<p>因为我本机已经安装了ollama 所以直接在cmd中执行</p>
<pre tabindex="0"><code>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</code></pre><p>这个时候在docker desktop的Images中就能看到下载的open webui了</p>
<p><img src="/images/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240712170056.png"></p>
<h2 id="加载大模型"><strong>加载大模型</strong></h2>
<p>我们在docker中点击run启动open-webui项目后在浏览器中输入<code>http://localhost:3000/</code>即可进入</p>
<p><img src="/images/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240712170710.png"></p>
<p>open-webui本身支持各种语言，英语困难的小伙伴也不用担心。点击右上角我的，设置里面可以选择语言，和主题风格。</p>
<p><img src="/images/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240712171102.png"></p>
<p>设置语言之后，我们在下面的管理员设置中可以管理我的大模型</p>
<p><img src="/images/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240712170947.png"></p>
<p>但是本人不建议在这里去下载我们的大模型，因为在这里下载会遇到卡在百分之99的情况。来自某人的教训。我们可以使用ollama提供的命令交互的方式去加载我们的大模型。</p>
<p>我们只需要在终端执行 便可以加载我们需要的大模型</p>
<pre tabindex="0"><code>ollama run llama3:8b
</code></pre><p>当然我们可以在ollama中选择我们想要下载那些大模型 <a href="https://ollama.com/library">https://ollama.com/library</a></p>
<p><img src="/images/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240712171817.png"></p>
<p>当我们加载完成大模型之后，我们就可以在本地愉快的和大模型进行对话了。</p>
<p><img src="/images/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20240712172055.png"></p>

            </div>
        </article>

        <hr />

        <div class="post-info">
            
            
  		</div>
    </main>

            </div>

            
                <footer class="footer">
    
    
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.85fad2de4f13fec3bcb3b3cb10430cdb44a7b4a9749b32938241a5c6e77718df7624f1002b880521fdc26e24ec1077fda214bf1cb36ee3045510760d09638cae.js" integrity="sha512-hfrS3k8T/sO8s7PLEEMM20SntKl0mzKTgkGlxud3GN92JPEAK4gFIf3CbiTsEHf9ohS/HLNu4wRVEHYNCWOMrg=="></script>




    </body>
</html>
